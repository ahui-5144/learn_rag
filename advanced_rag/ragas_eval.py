# RAGAS è¯„ä¼°è„šæœ¬
# ç”¨äºè¯„ä¼° b_rerank.py ä¸­çš„ä¸åŒé‡æ’åºæ–¹æ³•

# å®‰è£…ä¾èµ–
# pip install ragas datasets pandas

import os
import sys
import json
import pandas as pd
from dotenv import load_dotenv
from datasets import Dataset
from ragas import evaluate
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_recall,
    context_precision,
)

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# å¯¼å…¥ RAG ç³»ç»Ÿç»„ä»¶
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from b_rerank import (
    extract_text_from_pdf,
    chunk_text,
    create_embeddings,
    SimpleVectorStore,
    rag_with_reranking,
)

# ==================== é…ç½®åŒº ====================

# PDF æ–‡æ¡£è·¯å¾„
PDF_PATH = "../basic_rag/data/AI_Information.pdf"

# è¯„ä¼°æ•°æ®é›†æ–‡ä»¶
EVAL_DATASET_FILE = "eval_dataset.json"

# è¦è¯„ä¼°çš„æ–¹æ³•åˆ—è¡¨
METHODS_TO_EVALUATE = ["none", "llm", "keywords"]

# RAGAS ä½¿ç”¨çš„ LLMï¼ˆç”¨äºè¯„ä¼°ï¼Œé»˜è®¤ç”¨ OpenAIï¼Œä¹Ÿå¯é…ç½®å…¶ä»–ï¼‰
# RAGAS ä¼šè‡ªåŠ¨ä»ç¯å¢ƒå˜é‡è¯»å– OPENAI_API_KEY
# å¦‚æœä½¿ç”¨æ™ºè°±ï¼Œéœ€è¦åœ¨ä¸‹é¢å•ç‹¬é…ç½®


# ==================== è¯„ä¼°æ•°æ®é›† ====================

# ç¤ºä¾‹è¯„ä¼°æ•°æ®é›†ï¼ˆå¦‚æœä½¿ç”¨æ–‡ä»¶æ–¹å¼ï¼Œå¯ä»¥åˆ é™¤è¿™éƒ¨åˆ†ï¼‰
EVAL_DATASET = [
    {
        "question": "What is the difference between traditional programming and machine learning?",
        "ground_truth": "Traditional programming relies on explicit instructions written by programmers to perform tasks, while machine learning learns patterns from data to make decisions without being explicitly programmed for specific rules. In traditional programming, rules are coded by humans; in machine learning, rules are learned from data."
    },
    {
        "question": "What are the main components of an expert system?",
        "ground_truth": "Expert systems consist of two main components: a knowledge base, which stores facts and rules about a specific domain, and an inference engine, which applies logical rules to the knowledge base to derive new conclusions and make decisions."
    },
    {
        "question": "How does machine learning differ from deep learning?",
        "ground_truth": "Deep learning is a specialized subset of machine learning that uses multi-layered neural networks to learn from data. While traditional machine learning often requires manual feature engineering, deep learning can automatically learn features from raw data."
    },
    {
        "question": "What is the role of training data in machine learning?",
        "ground_truth": "Training data in machine learning serves as the example set from which the algorithm learns patterns, relationships, and rules. The quality and quantity of training data directly affect the model's performance and ability to generalize to new, unseen data."
    },
    {
        "question": "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
        "ground_truth": "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ƒä½¿ç”¨ç®—æ³•ä»æ•°æ®ä¸­å­¦ä¹ æ¨¡å¼ï¼Œå¹¶åˆ©ç”¨è¿™äº›æ¨¡å¼åšå‡ºé¢„æµ‹æˆ–å†³ç­–ï¼Œè€Œæ— éœ€ä¸ºç‰¹å®šè§„åˆ™è¿›è¡Œæ˜¾å¼ç¼–ç¨‹ã€‚"
    }
]


# ==================== å·¥å…·å‡½æ•° ====================

def load_eval_dataset(file_path=None):
    """
    åŠ è½½è¯„ä¼°æ•°æ®é›†

    Args:
        file_path: JSON æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸º None åˆ™ä½¿ç”¨å†…ç½®ç¤ºä¾‹æ•°æ®

    Returns:
        list: è¯„ä¼°æ•°æ®åˆ—è¡¨
    """
    if file_path and os.path.exists(file_path):
        with open(file_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    else:
        print(f"ä½¿ç”¨å†…ç½®è¯„ä¼°æ•°æ®é›†ï¼ˆ{len(EVAL_DATASET)} æ¡ï¼‰")
        return EVAL_DATASET


def init_rag_system(pdf_path):
    """
    åˆå§‹åŒ– RAG ç³»ç»Ÿ

    Args:
        pdf_path: PDF æ–‡æ¡£è·¯å¾„

    Returns:
        SimpleVectorStore: åˆå§‹åŒ–å¥½çš„å‘é‡å­˜å‚¨
    """
    print(f"\næ­£åœ¨åŠ è½½æ–‡æ¡£: {pdf_path}")

    # æå–æ–‡æœ¬
    text = extract_text_from_pdf(pdf_path)

    # åˆ†å—
    chunks = chunk_text(text, 1000, 200)
    print(f"åˆ›å»ºäº† {len(chunks)} ä¸ªåˆ†å—")

    # åˆ›å»ºå‘é‡å­˜å‚¨
    embeddings = create_embeddings(chunks)
    vector_store = SimpleVectorStore()

    for chunk, embedding in zip(chunks, embeddings):
        vector_store.add_item(
            text=chunk,
            embedding=embedding,
            metadata={"source": pdf_path}
        )

    print(f"å‘é‡å­˜å‚¨å·²å»ºç«‹ï¼ŒåŒ…å« {len(chunks)} ä¸ªå—")

    return vector_store


def evaluate_single_method(method_name, eval_dataset, vector_store, model="GLM-4.5"):
    """
    è¯„ä¼°å•ä¸ªé‡æ’åºæ–¹æ³•

    Args:
        method_name: é‡æ’åºæ–¹æ³•åç§° ("none", "llm", "keywords")
        eval_dataset: è¯„ä¼°æ•°æ®é›†
        vector_store: å‘é‡å­˜å‚¨
        model: LLM æ¨¡å‹åç§°

    Returns:
        dict: åŒ…å«é—®é¢˜å’Œç»“æœçš„æ•°æ®
    """
    print(f"\n{'='*60}")
    print(f"æ­£åœ¨è¯„ä¼°æ–¹æ³•: {method_name}")
    print('='*60)

    results = {
        "question": [],
        "ground_truth": [],
        "answer": [],
        "contexts": [],
    }

    for i, item in enumerate(eval_dataset, 1):
        question = item["question"]
        ground_truth = item["ground_truth"]

        print(f"\n[{i}/{len(eval_dataset)}] é—®é¢˜: {question[:50]}...")

        try:
            # è°ƒç”¨ RAG ç³»ç»Ÿ
            response = rag_with_reranking(
                query=question,
                vector_store=vector_store,
                reranking_method=method_name,
                top_n=3,
                model=model
            )

            # æå–ä¸Šä¸‹æ–‡æ–‡æœ¬
            contexts = [r["text"] for r in response["reranked_results"]]

            # ä¿å­˜ç»“æœ
            results["question"].append(question)
            results["ground_truth"].append(ground_truth)
            results["answer"].append(response["response"])
            results["contexts"].append(contexts)

            print(f"  âœ“ ç­”æ¡ˆç”Ÿæˆå®Œæˆ (æ£€ç´¢åˆ° {len(contexts)} ä¸ªä¸Šä¸‹æ–‡)")

        except Exception as e:
            print(f"  âœ— å¤„ç†å¤±è´¥: {e}")
            # å¤±è´¥æ—¶æ·»åŠ ç©ºç»“æœ
            results["question"].append(question)
            results["ground_truth"].append(ground_truth)
            results["answer"].append(f"Error: {str(e)}")
            results["contexts"].append([])

    return results


def run_ragas_evaluation(results_dict):
    """
    ä½¿ç”¨ RAGAS è¿è¡Œè¯„ä¼°

    Args:
        results_dict: åŒ…å« question, ground_truth, answer, contexts çš„å­—å…¸

    Returns:
        Dataset: RAGAS è¯„ä¼°ç»“æœ
    """
    print("\næ­£åœ¨è¿è¡Œ RAGAS è¯„ä¼°...")

    # è½¬æ¢ä¸º RAGAS Dataset æ ¼å¼
    dataset = Dataset.from_dict(results_dict)

    # è¿è¡Œè¯„ä¼°
    score = evaluate(
        dataset=dataset,
        metrics=[
            faithfulness,
            answer_relevancy,
            context_precision,
            context_recall,
        ]
    )

    return score


def save_results(all_scores, all_results, output_file="ragas_evaluation_results.json"):
    """
    ä¿å­˜è¯„ä¼°ç»“æœåˆ° JSON æ–‡ä»¶

    Args:
        all_scores: å„æ–¹æ³•çš„ RAGAS è¯„åˆ†
        all_results: å„æ–¹æ³•çš„è¯¦ç»†ç»“æœ
        output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
    """
    output_data = {
        "summary": {},
        "details": {}
    }

    # ä¿å­˜è¯„åˆ†æ‘˜è¦
    for method, score in all_scores.items():
        score_df = score.to_pandas()
        output_data["summary"][method] = {
            "faithfulness": float(score_df["faithfulness"].mean()),
            "answer_relevancy": float(score_df["answer_relevancy"].mean()),
            "context_precision": float(score_df["context_precision"].mean()),
            "context_recall": float(score_df["context_recall"].mean()),
        }

    # ä¿å­˜è¯¦ç»†ç»“æœ
    for method, results in all_results.items():
        output_data["details"][method] = results

    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)

    print(f"\nç»“æœå·²ä¿å­˜åˆ°: {output_file}")


def print_comparison_table(all_scores):
    """
    æ‰“å°æ–¹æ³•å¯¹æ¯”è¡¨æ ¼

    Args:
        all_scores: å„æ–¹æ³•çš„ RAGAS è¯„åˆ†å­—å…¸
    """
    print(f"\n{'='*80}")
    print(" " * 25 + "æ–¹æ³•å¯¹æ¯”æ€»ç»“")
    print('='*80)

    comparison_data = []

    for method, score in all_scores.items():
        score_df = score.to_pandas()

        row = {
            "Method": method.upper(),
            "Faithfulness": f"{score_df['faithfulness'].mean():.4f}",
            "Answer Relevancy": f"{score_df['answer_relevancy'].mean():.4f}",
            "Context Precision": f"{score_df['context_precision'].mean():.4f}",
            "Context Recall": f"{score_df['context_recall'].mean():.4f}",
        }
        comparison_data.append(row)

    # æ‰“å°è¡¨æ ¼
    df = pd.DataFrame(comparison_data)
    print(df.to_string(index=False))

    print('\n' + '='*80)
    print("æŒ‡æ ‡è¯´æ˜:")
    print("  â€¢ Faithfulness (å¿ å®åº¦): ç­”æ¡ˆæ˜¯å¦å¿ å®äºæ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡")
    print("  â€¢ Answer Relevancy (ç­”æ¡ˆç›¸å…³æ€§): ç­”æ¡ˆä¸é—®é¢˜çš„ç›¸å…³ç¨‹åº¦")
    print("  â€¢ Context Precision (ä¸Šä¸‹æ–‡ç²¾ç¡®åº¦): æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ç›¸å…³æ€§")
    print("  â€¢ Context Recall (ä¸Šä¸‹æ–‡å¬å›ç‡): æ£€ç´¢æ˜¯å¦è¦†ç›–æ ‡å‡†ç­”æ¡ˆæ‰€éœ€ä¿¡æ¯")
    print('='*80)


def print_best_method(all_scores):
    """
    æ‰“å°æœ€ä½³æ–¹æ³•å’Œæ¨è

    Args:
        all_scores: å„æ–¹æ³•çš„ RAGAS è¯„åˆ†å­—å…¸
    """
    # è®¡ç®—æ¯ä¸ªæ–¹æ³•çš„å¹³å‡åˆ†
    avg_scores = {}
    for method, score in all_scores.items():
        score_df = score.to_pandas()
        avg_scores[method] = score_df[["faithfulness", "answer_relevancy",
                                       "context_precision", "context_recall"]].mean().mean()

    # æ‰¾å‡ºæœ€ä½³æ–¹æ³•
    best_method = max(avg_scores, key=avg_scores.get)

    print(f"\n{'='*80}")
    print(f"ğŸ† æœ€ä½³æ–¹æ³•: {best_method.upper()}")
    print(f"   ç»¼åˆå¾—åˆ†: {avg_scores[best_method]:.4f}")
    print('='*80)

    # æ‰“å°æ¨èå»ºè®®
    print("\nğŸ’¡ ä½¿ç”¨å»ºè®®:")

    if best_method == "llm":
        print("   â€¢ LLM é‡æ’åºæ•ˆæœæœ€å¥½ï¼Œé€‚åˆé«˜ä»·å€¼åœºæ™¯")
        print("   â€¢ ç¼ºç‚¹æ˜¯éœ€è¦é¢å¤–çš„ API è°ƒç”¨ï¼Œæˆæœ¬è¾ƒé«˜")
        print("   â€¢ å¯¹äºç®€å•æŸ¥è¯¢ï¼Œå¯ä»¥è€ƒè™‘ä½¿ç”¨ keywords æ–¹æ³•ä»¥é™ä½æˆæœ¬")

    elif best_method == "keywords":
        print("   â€¢ å…³é”®è¯é‡æ’åºæ€§ä»·æ¯”é«˜ï¼Œæ— éœ€é¢å¤– API è°ƒç”¨")
        print("   â€¢ é€‚åˆå…³é”®è¯æ˜ç¡®ã€æœ¯è¯­è§„èŒƒçš„åœºæ™¯")
        print("   â€¢ å¯¹äºè¯­ä¹‰å¤æ‚çš„é—®é¢˜ï¼Œå¯èƒ½éœ€è¦ç»“åˆ LLM é‡æ’åº")

    elif best_method == "none":
        print("   â€¢ åŸå§‹å‘é‡æ£€ç´¢æ•ˆæœå·²ç»å¾ˆå¥½")
        print("   â€¢ é‡æ’åºå¯èƒ½åœ¨æ­¤åœºæ™¯ä¸‹æå‡æœ‰é™")
        print("   â€¢ å»ºè®®æ£€æŸ¥è¯„ä¼°æ•°æ®é›†æ˜¯å¦è¿‡äºç®€å•")


# ==================== ä¸»å‡½æ•° ====================

def main():
    """ä¸»å‡½æ•°ï¼šæ‰§è¡Œå®Œæ•´çš„è¯„ä¼°æµç¨‹"""

    print("\n" + "="*80)
    print(" " * 25 + "RAGAS è¯„ä¼°è„šæœ¬")
    print("="*80)

    # 1. åŠ è½½è¯„ä¼°æ•°æ®é›†
    eval_dataset = load_eval_dataset(EVAL_DATASET_FILE)
    print(f"\nåŠ è½½äº† {len(eval_dataset)} æ¡è¯„ä¼°æ•°æ®")

    # 2. åˆå§‹åŒ– RAG ç³»ç»Ÿ
    vector_store = init_rag_system(PDF_PATH)

    # 3. è¯„ä¼°å„ä¸ªæ–¹æ³•
    all_scores = {}
    all_results = {}

    for method in METHODS_TO_EVALUATE:
        # è¯„ä¼°å•ä¸ªæ–¹æ³•
        results = evaluate_single_method(method, eval_dataset, vector_store)
        all_results[method] = results

        # è¿è¡Œ RAGAS è¯„ä¼°
        score = run_ragas_evaluation(results)
        all_scores[method] = score

        # æ‰“å°å•ä¸ªæ–¹æ³•çš„è¯„åˆ†
        score_df = score.to_pandas()
        print(f"\n{method.upper()} æ–¹æ³•è¯„åˆ†:")
        print(score_df.to_string(index=False))

    # 4. æ‰“å°å¯¹æ¯”è¡¨æ ¼
    print_comparison_table(all_scores)

    # 5. æ‰“å°æœ€ä½³æ–¹æ³•æ¨è
    print_best_method(all_scores)

    # 6. ä¿å­˜ç»“æœ
    save_results(all_scores, all_results)

    print("\nâœ“ è¯„ä¼°å®Œæˆ!")


if __name__ == "__main__":
    main()
